{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1lbeMyKRxN9H4gvHfq4d67IRcAKiUiG6k",
      "authorship_tag": "ABX9TyNWbyRKfAOUKVBemZ9QO4RK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnriqueProjectsIA/Pytorch-teaching/blob/main/LSTM_for_NLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "d10Ere8TENBm"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/Data_sets/sometext.txt','r', encoding='utf8') as handle:\n",
        "  text = handle.read()"
      ],
      "metadata": {
        "id": "Kktqlw29E3Cu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now can review the text"
      ],
      "metadata": {
        "id": "fRINjSFWMpAu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 192
        },
        "id": "FFp5dd7oMcwL",
        "outputId": "ffa22e0a-ebd4-4d0e-8460-27393bd29bb6"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bud buriest thy content,\\n  And tender churl mak'st waste in niggarding:\\n    Pity the world, or else this glutton be,\\n    To eat the world's due, by the grave and thee.\\n\\n\\n                     2\\n  When forty winters shall besiege thy brow,\\n  And dig deep trenches in thy beauty's field,\\n  Thy youth's proud livery so gazed on now,\\n  Will be a tattered weed of small worth held:  \\n  Then being asked, where all thy beauty lies,\\n  Where all the treasure of thy lusty days;\\n  To say within thine own deep su\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhUt9t2AMrns",
        "outputId": "4c7b86be-5734-48db-f072-a5dbe8b28208"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5445609"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this approach, we will go character level. That means we hot encode characters. "
      ],
      "metadata": {
        "id": "qZ_m6EkyM-Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "allCarc = set(text)"
      ],
      "metadata": {
        "id": "88-HSZ50M5Fc"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We build and encoder an a decoder:\n",
        "\n",
        "encoder: Letter->Number\n",
        "\n",
        "decoder: Number->Letter"
      ],
      "metadata": {
        "id": "lhtIoyWjFrrq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = dict(enumerate(allCarc))\n",
        "encoder = {char:ind for ind,char in decoder.items()}"
      ],
      "metadata": {
        "id": "r4fu6nhTNUd9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "0kxLpP3XGHW7",
        "outputId": "60a47ee2-cc43-4502-83ab-d7d160394721"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'M'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder['w']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXAuNeJLGhwJ",
        "outputId": "c1169748-e9bb-4bd5-981d-71d44f0abfd0"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "39"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can encode the text"
      ],
      "metadata": {
        "id": "-4HNc7GkHBJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "encodeText = np.array([encoder[char] for char in text])"
      ],
      "metadata": {
        "id": "Pyyux0dIGmjq"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encodeText[:500]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-koX6vvHNxh",
        "outputId": "0ecd9d4c-481e-4b53-80ea-e0cb83a25c5c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([59, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47, 47,\n",
              "       47, 47, 47, 47, 47, 72, 59, 47, 47, 68, 12, 67, 31, 47, 80, 27, 52,\n",
              "       12, 60,  4,  8, 47, 58, 12, 60, 27,  8, 70, 12, 60,  4, 47, 39, 60,\n",
              "       47, 30, 60,  4, 52, 12, 60, 47, 52,  6, 58, 12, 60, 27,  4, 60, 21,\n",
              "       59, 47, 47, 75, 50, 27,  8, 47,  8, 50, 60, 12, 60, 17, 20, 47, 17,\n",
              "       60, 27, 70,  8, 20,  7,  4, 47, 12, 67,  4, 60, 47, 31, 52, 40, 50,\n",
              "        8, 47,  6, 60, 61, 60, 12, 47, 30, 52, 60, 21, 59, 47, 47, 57, 70,\n",
              "        8, 47, 27,  4, 47,  8, 50, 60, 47, 12, 52, 18, 60, 12, 47,  4, 50,\n",
              "       67, 70, 53, 30, 47, 17, 20, 47,  8, 52, 31, 60, 47, 30, 60, 58, 60,\n",
              "       27,  4, 60, 21, 59, 47, 47, 65, 52,  4, 47,  8, 60,  6, 30, 60, 12,\n",
              "       47, 50, 60, 52, 12, 47, 31, 52, 40, 50,  8, 47, 17, 60, 27, 12, 47,\n",
              "       50, 52,  4, 47, 31, 60, 31, 67, 12, 20, 37, 59, 47, 47, 57, 70,  8,\n",
              "       47,  8, 50, 67, 70, 47, 58, 67,  6,  8, 12, 27, 58,  8, 60, 30, 47,\n",
              "        8, 67, 47,  8, 50, 52,  6, 60, 47, 67, 39,  6, 47, 17, 12, 52, 40,\n",
              "       50,  8, 47, 60, 20, 60,  4, 21, 59, 47, 47, 68, 60, 60, 30,  7,  4,\n",
              "        8, 47,  8, 50, 20, 47, 53, 52, 40, 50,  8,  7,  4, 47, 80, 53, 27,\n",
              "       31, 60, 47, 39, 52,  8, 50, 47,  4, 60, 53, 80, 79,  4, 70, 17,  4,\n",
              "        8, 27,  6,  8, 52, 27, 53, 47, 80, 70, 60, 53, 21, 59, 47, 47,  0,\n",
              "       27, 78, 52,  6, 40, 47, 27, 47, 80, 27, 31, 52,  6, 60, 47, 39, 50,\n",
              "       60, 12, 60, 47, 27, 17, 70,  6, 30, 27,  6, 58, 60, 47, 53, 52, 60,\n",
              "        4, 21, 59, 47, 47, 75, 50, 20, 47,  4, 60, 53, 80, 47,  8, 50, 20,\n",
              "       47, 80, 67, 60, 21, 47,  8, 67, 47,  8, 50, 20, 47,  4, 39, 60, 60,\n",
              "        8, 47,  4, 60, 53, 80, 47,  8, 67, 67, 47, 58, 12, 70, 60, 53, 37,\n",
              "       59, 47, 47, 75, 50, 67, 70, 47,  8, 50, 27,  8, 47, 27, 12,  8, 47,\n",
              "        6, 67, 39, 47,  8, 50, 60, 47, 39, 67, 12, 53, 30,  7,  4, 47, 80,\n",
              "       12, 60,  4, 50, 47, 67, 12,  6, 27, 31, 60,  6,  8, 21, 59, 47, 47,\n",
              "       13,  6, 30, 47, 67,  6, 53, 20, 47, 50, 60, 12, 27, 53, 30, 47,  8,\n",
              "       67, 47,  8, 50, 60, 47, 40, 27, 70, 30, 20, 47,  4, 18, 12, 52,  6,\n",
              "       40, 21, 59, 47, 47, 11, 52,  8, 50, 52,  6, 47,  8, 50, 52,  6, 60,\n",
              "       47, 67, 39,  6, 47, 17, 70])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoder[29]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "tGbDj_laHRn5",
        "outputId": "b66d4f6c-5095-4596-e04b-fd7ea233fea9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'8'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the array, one can create one hot encode of the text"
      ],
      "metadata": {
        "id": "JrAjUvHkHcox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot_encoder(encoded_text, num_uni_chars):\n",
        "    '''\n",
        "    encoded_text : batch of encoded text\n",
        "    \n",
        "    num_uni_chars = number of unique characters (len(set(text)))\n",
        "    '''\n",
        "    \n",
        "    # METHOD FROM:\n",
        "    # https://stackoverflow.com/questions/29831489/convert-encoded_textay-of-indices-to-1-hot-encoded-numpy-encoded_textay\n",
        "      \n",
        "    # Create a placeholder for zeros.\n",
        "    one_hot = np.zeros((encoded_text.size, num_uni_chars))\n",
        "    \n",
        "    # Convert data type for later use with pytorch (errors if we dont!)\n",
        "    one_hot = one_hot.astype(np.float32)\n",
        "\n",
        "    # Using fancy indexing fill in the 1s at the correct index locations\n",
        "    one_hot[np.arange(one_hot.shape[0]), encoded_text.flatten()] = 1.0\n",
        "    \n",
        "\n",
        "    # Reshape it so it matches the batch sahe\n",
        "    one_hot = one_hot.reshape((*encoded_text.shape, num_uni_chars))\n",
        "    \n",
        "    return one_hot"
      ],
      "metadata": {
        "id": "XwnzjUSDoCeE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To check the function"
      ],
      "metadata": {
        "id": "fP7rt6RULA4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "foo = np.array([1,2,0])\n",
        "one_hot_encoder(foo,3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6344BFLSK495",
        "outputId": "587fc099-3da2-4fb8-dfe5-05ad69b5c0d7"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0.],\n",
              "       [0., 0., 1.],\n",
              "       [1., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Batch"
      ],
      "metadata": {
        "id": "LRcbHj4zSlKU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_batches(encoded_text, samples_per_batch = 10, sequence_len = 50):\n",
        "  #X: encoded text of length = sequence_len\n",
        "  #Y: encoded text shifted by one\n",
        "\n",
        "  #How many chars per batch?\n",
        "  char_per_batch = samples_per_batch*sequence_len\n",
        "\n",
        "  #How many batches can we make, given the length of the encoded text?\n",
        "  num_batches_avail = int(len(encoded_text)/char_per_batch)\n",
        "\n",
        "  # Eliminate the section of the text that won't fit evenly into a batch\n",
        "  encoded_text = encoded_text[:num_batches_avail*char_per_batch]\n",
        "\n",
        "  encoded_text = encoded_text.reshape((samples_per_batch,-1))\n",
        "\n",
        "  for n in range(0, encoded_text.shape[1], sequence_len):\n",
        "    x = encoded_text[:,n:n+sequence_len]\n",
        "\n",
        "    y = np.zeros_like(x)\n",
        "\n",
        "    try:\n",
        "      y[:,:-1] = x[:,1:]\n",
        "      y[:,-1] = encoded_text[:,n+sequence_len]\n",
        "    except:\n",
        "      y[:,:-1] = x[:,1:]\n",
        "      y[:,-1] = encoded_text[:,0]\n",
        "\n",
        "    yield x,y"
      ],
      "metadata": {
        "id": "cgYYSadFSzlW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampleText = np.arange(20)\n",
        "batchGenerator = generate_batches(sampleText, 2,5)"
      ],
      "metadata": {
        "id": "fF74PnheU-fV"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x,y = next(batchGenerator)"
      ],
      "metadata": {
        "id": "FKSlYzj1X23f"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8piAMFacYjAU",
        "outputId": "254e37f0-1498-4463-bf8c-f7c9b3795ef3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0,  1,  2,  3,  4],\n",
              "       [10, 11, 12, 13, 14]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iP-sYzgYjYT",
        "outputId": "782ffa74-fbdd-4ca8-cbbb-c4c7d4779d94"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  3,  4,  5],\n",
              "       [11, 12, 13, 14, 15]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM implementation"
      ],
      "metadata": {
        "id": "TpBk5YxTP5qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CharModel(nn.Module):\n",
        "  def __init__(self,all_chars,num_hidden = 256, num_layers = 4,drop_out=0.5, use_gpu=False):\n",
        "\n",
        "    super().__init__()\n",
        "    self.drop_prob = drop_out\n",
        "    self.num_layers = num_layers\n",
        "    self.num_hidden = num_hidden\n",
        "    self.use_gpu = use_gpu\n",
        "\n",
        "    self.all_chars = all_chars\n",
        "    self.decoder = dict(enumerate(all_chars))\n",
        "    self.encoder = {char:ind for ind,char in decoder.items()}\n",
        "\n",
        "    self.lstm = nn.LSTM(len(self.all_chars),num_hidden,num_layers, dropout = drop_out, batch_first=True)\n",
        "    self.dropout = nn.Dropout(drop_out)\n",
        "    self.fc_linear = nn.Linear(num_hidden, len(self.all_chars))\n",
        "\n",
        "  def forward(self,x,hidden):\n",
        "\n",
        "    lstm_output, hidden = self.lstm(x,hidden)\n",
        "\n",
        "    drop_output = self.dropout(lstm_output)\n",
        "\n",
        "    drop_output =drop_output.contiguous().view(-1, self.num_hidden)\n",
        "\n",
        "    final_out = self.fc_linear(drop_output)\n",
        "\n",
        "    return final_out, hidden\n",
        "\n",
        "\n",
        "  def hidden_state(self,batch_size):\n",
        "    if self.use_gpu:\n",
        "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda(),\n",
        "                torch.zeros(self.num_layers, batch_size, self.num_hidden).cuda())\n",
        "    else:\n",
        "      hidden = (torch.zeros(self.num_layers, batch_size, self.num_hidden),\n",
        "                torch.zeros(self.num_layers, batch_size, self.num_hidden))\n",
        "    return hidden"
      ],
      "metadata": {
        "id": "lsqSzYdKP9hq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CharModel(all_chars=allCarc,\n",
        "                  num_hidden=512,\n",
        "                  num_layers=3,\n",
        "                  drop_out=0.5,\n",
        "                  use_gpu = True)"
      ],
      "metadata": {
        "id": "aD_sa078V0Nk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_param = [int(p.numel()) for p in model.parameters()]\n",
        "total_param"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-dzGfG2WROU",
        "outputId": "e072cf5f-7ccb-407f-8cf7-00319ad99caf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[172032,\n",
              " 1048576,\n",
              " 2048,\n",
              " 2048,\n",
              " 1048576,\n",
              " 1048576,\n",
              " 2048,\n",
              " 2048,\n",
              " 1048576,\n",
              " 1048576,\n",
              " 2048,\n",
              " 2048,\n",
              " 43008,\n",
              " 84]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* As a rule of thumb, one wants to have as many parameters as characters in the dataset. Because, if we have too many parameters it is possible to overfit the data."
      ],
      "metadata": {
        "id": "6HIVLXzhYhxe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sum(total_param)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B_Ft8xYSX_YD",
        "outputId": "16b1ca36-9071-4892-dbdd-2c3d92ba5748"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5470292"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "fJX5pdITYHYD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pecent = 0.9\n",
        "train_ind = int(len(encodeText)*train_pecent)\n",
        "train_data = encodeText[:train_ind]\n",
        "val_data = encodeText[train_ind:]\n",
        "len(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TokuKQOqY6aU",
        "outputId": "99dc7c8f-bfde-4fe4-be26-c6febc5d34e1"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4901048"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjZDQ2zjZa8k",
        "outputId": "f039ced3-2742-4f89-f877-0a782707b064"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "544561"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 13\n",
        "batch_size = 100\n",
        "\n",
        "seq_len = 100\n",
        "\n",
        "tracker = 0\n",
        "\n",
        "num_char = max(encodeText)+1\n",
        "\n",
        "# Set model to train\n",
        "model.train()\n",
        "\n",
        "\n",
        "# Check to see if using GPU\n",
        "if model.use_gpu:\n",
        "    model.cuda()\n",
        "\n",
        "for i in range(epochs):\n",
        "    \n",
        "    hidden = model.hidden_state(batch_size)\n",
        "    \n",
        "    \n",
        "    for x,y in generate_batches(train_data,batch_size,seq_len):\n",
        "        \n",
        "        tracker += 1\n",
        "        \n",
        "        # One Hot Encode incoming data\n",
        "        x = one_hot_encoder(x,num_char)\n",
        "        \n",
        "        # Convert Numpy Arrays to Tensor\n",
        "        \n",
        "        inputs = torch.from_numpy(x)\n",
        "        targets = torch.from_numpy(y)\n",
        "        \n",
        "        # Adjust for GPU if necessary\n",
        "        \n",
        "        if model.use_gpu:\n",
        "            \n",
        "            inputs = inputs.cuda()\n",
        "            targets = targets.cuda()\n",
        "            \n",
        "        # Reset Hidden State\n",
        "        # If we dont' reset we would backpropagate through all training history\n",
        "        hidden = tuple([state.data for state in hidden])\n",
        "        \n",
        "        model.zero_grad()\n",
        "        \n",
        "        lstm_output, hidden = model.forward(inputs,hidden)\n",
        "        loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        # POSSIBLE EXPLODING GRADIENT PROBLEM!\n",
        "        # LET\"S CLIP JUST IN CASE\n",
        "        nn.utils.clip_grad_norm_(model.parameters(),max_norm=5)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        \n",
        "        ###################################\n",
        "        ### CHECK ON VALIDATION SET ######\n",
        "        #################################\n",
        "        \n",
        "        if tracker % 25 == 0:\n",
        "            \n",
        "            val_hidden = model.hidden_state(batch_size)\n",
        "            val_losses = []\n",
        "            model.eval()\n",
        "            \n",
        "            for x,y in generate_batches(val_data,batch_size,seq_len):\n",
        "                \n",
        "                # One Hot Encode incoming data\n",
        "                x = one_hot_encoder(x,num_char)\n",
        "                \n",
        "\n",
        "                # Convert Numpy Arrays to Tensor\n",
        "\n",
        "                inputs = torch.from_numpy(x)\n",
        "                targets = torch.from_numpy(y)\n",
        "\n",
        "                # Adjust for GPU if necessary\n",
        "\n",
        "                if model.use_gpu:\n",
        "\n",
        "                    inputs = inputs.cuda()\n",
        "                    targets = targets.cuda()\n",
        "                    \n",
        "                # Reset Hidden State\n",
        "                # If we dont' reset we would backpropagate through \n",
        "                # all training history\n",
        "                val_hidden = tuple([state.data for state in val_hidden])\n",
        "                \n",
        "                lstm_output, val_hidden = model.forward(inputs,val_hidden)\n",
        "                val_loss = criterion(lstm_output,targets.view(batch_size*seq_len).long())\n",
        "        \n",
        "                val_losses.append(val_loss.item())\n",
        "            \n",
        "            # Reset to training model after val for loop\n",
        "            model.train()\n",
        "            \n",
        "            print(f\"Epoch: {i} Step: {tracker} Val Loss: {val_loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QLSEepPLo2LZ",
        "outputId": "9e3e026f-0326-4162-98d3-b8ec99f95648"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Step: 25 Val Loss: 3.20159912109375\n",
            "Epoch: 0 Step: 50 Val Loss: 3.1937050819396973\n",
            "Epoch: 0 Step: 75 Val Loss: 6.58789587020874\n",
            "Epoch: 0 Step: 100 Val Loss: 3.1885204315185547\n",
            "Epoch: 0 Step: 125 Val Loss: 3.08390474319458\n",
            "Epoch: 0 Step: 150 Val Loss: 2.9729442596435547\n",
            "Epoch: 0 Step: 175 Val Loss: 2.856128215789795\n",
            "Epoch: 0 Step: 200 Val Loss: 2.716707706451416\n",
            "Epoch: 0 Step: 225 Val Loss: 2.632988929748535\n",
            "Epoch: 0 Step: 250 Val Loss: 2.531026840209961\n",
            "Epoch: 0 Step: 275 Val Loss: 2.406292676925659\n",
            "Epoch: 0 Step: 300 Val Loss: 2.30350399017334\n",
            "Epoch: 0 Step: 325 Val Loss: 2.2238659858703613\n",
            "Epoch: 0 Step: 350 Val Loss: 2.1682865619659424\n",
            "Epoch: 0 Step: 375 Val Loss: 2.116222381591797\n",
            "Epoch: 0 Step: 400 Val Loss: 2.0757057666778564\n",
            "Epoch: 0 Step: 425 Val Loss: 2.0295915603637695\n",
            "Epoch: 0 Step: 450 Val Loss: 2.010106325149536\n",
            "Epoch: 0 Step: 475 Val Loss: 1.9710310697555542\n",
            "Epoch: 1 Step: 500 Val Loss: 1.9496560096740723\n",
            "Epoch: 1 Step: 525 Val Loss: 1.92460036277771\n",
            "Epoch: 1 Step: 550 Val Loss: 1.8964345455169678\n",
            "Epoch: 1 Step: 575 Val Loss: 1.875091552734375\n",
            "Epoch: 1 Step: 600 Val Loss: 1.8548831939697266\n",
            "Epoch: 1 Step: 625 Val Loss: 1.8375877141952515\n",
            "Epoch: 1 Step: 650 Val Loss: 1.8101894855499268\n",
            "Epoch: 1 Step: 675 Val Loss: 1.8044277429580688\n",
            "Epoch: 1 Step: 700 Val Loss: 1.782128930091858\n",
            "Epoch: 1 Step: 725 Val Loss: 1.7655730247497559\n",
            "Epoch: 1 Step: 750 Val Loss: 1.7549011707305908\n",
            "Epoch: 1 Step: 775 Val Loss: 1.7436611652374268\n",
            "Epoch: 1 Step: 800 Val Loss: 1.7283746004104614\n",
            "Epoch: 1 Step: 825 Val Loss: 1.7115970849990845\n",
            "Epoch: 1 Step: 850 Val Loss: 1.7022205591201782\n",
            "Epoch: 1 Step: 875 Val Loss: 1.6957160234451294\n",
            "Epoch: 1 Step: 900 Val Loss: 1.6837947368621826\n",
            "Epoch: 1 Step: 925 Val Loss: 1.6731228828430176\n",
            "Epoch: 1 Step: 950 Val Loss: 1.6664173603057861\n",
            "Epoch: 1 Step: 975 Val Loss: 1.6498392820358276\n",
            "Epoch: 2 Step: 1000 Val Loss: 1.6564773321151733\n",
            "Epoch: 2 Step: 1025 Val Loss: 1.644193172454834\n",
            "Epoch: 2 Step: 1050 Val Loss: 1.630441427230835\n",
            "Epoch: 2 Step: 1075 Val Loss: 1.6192219257354736\n",
            "Epoch: 2 Step: 1100 Val Loss: 1.611642599105835\n",
            "Epoch: 2 Step: 1125 Val Loss: 1.5998778343200684\n",
            "Epoch: 2 Step: 1150 Val Loss: 1.5947668552398682\n",
            "Epoch: 2 Step: 1175 Val Loss: 1.5842914581298828\n",
            "Epoch: 2 Step: 1200 Val Loss: 1.577839732170105\n",
            "Epoch: 2 Step: 1225 Val Loss: 1.5728561878204346\n",
            "Epoch: 2 Step: 1250 Val Loss: 1.570752739906311\n",
            "Epoch: 2 Step: 1275 Val Loss: 1.5617659091949463\n",
            "Epoch: 2 Step: 1300 Val Loss: 1.555355429649353\n",
            "Epoch: 2 Step: 1325 Val Loss: 1.5526032447814941\n",
            "Epoch: 2 Step: 1350 Val Loss: 1.5462284088134766\n",
            "Epoch: 2 Step: 1375 Val Loss: 1.5469822883605957\n",
            "Epoch: 2 Step: 1400 Val Loss: 1.5385961532592773\n",
            "Epoch: 2 Step: 1425 Val Loss: 1.5368914604187012\n",
            "Epoch: 2 Step: 1450 Val Loss: 1.5329476594924927\n",
            "Epoch: 3 Step: 1475 Val Loss: 1.5247162580490112\n",
            "Epoch: 3 Step: 1500 Val Loss: 1.5180877447128296\n",
            "Epoch: 3 Step: 1525 Val Loss: 1.5123728513717651\n",
            "Epoch: 3 Step: 1550 Val Loss: 1.514693021774292\n",
            "Epoch: 3 Step: 1575 Val Loss: 1.506349802017212\n",
            "Epoch: 3 Step: 1600 Val Loss: 1.4993935823440552\n",
            "Epoch: 3 Step: 1625 Val Loss: 1.4944347143173218\n",
            "Epoch: 3 Step: 1650 Val Loss: 1.4910885095596313\n",
            "Epoch: 3 Step: 1675 Val Loss: 1.485654354095459\n",
            "Epoch: 3 Step: 1700 Val Loss: 1.481260895729065\n",
            "Epoch: 3 Step: 1725 Val Loss: 1.4772332906723022\n",
            "Epoch: 3 Step: 1750 Val Loss: 1.4746766090393066\n",
            "Epoch: 3 Step: 1775 Val Loss: 1.476912498474121\n",
            "Epoch: 3 Step: 1800 Val Loss: 1.4715524911880493\n",
            "Epoch: 3 Step: 1825 Val Loss: 1.4748876094818115\n",
            "Epoch: 3 Step: 1850 Val Loss: 1.4798957109451294\n",
            "Epoch: 3 Step: 1875 Val Loss: 1.4721546173095703\n",
            "Epoch: 3 Step: 1900 Val Loss: 1.463870644569397\n",
            "Epoch: 3 Step: 1925 Val Loss: 1.46065092086792\n",
            "Epoch: 3 Step: 1950 Val Loss: 1.4542617797851562\n",
            "Epoch: 4 Step: 1975 Val Loss: 1.4561115503311157\n",
            "Epoch: 4 Step: 2000 Val Loss: 1.4558011293411255\n",
            "Epoch: 4 Step: 2025 Val Loss: 1.4586387872695923\n",
            "Epoch: 4 Step: 2050 Val Loss: 1.4433937072753906\n",
            "Epoch: 4 Step: 2075 Val Loss: 1.4449689388275146\n",
            "Epoch: 4 Step: 2100 Val Loss: 1.4428229331970215\n",
            "Epoch: 4 Step: 2125 Val Loss: 1.4384896755218506\n",
            "Epoch: 4 Step: 2150 Val Loss: 1.4320769309997559\n",
            "Epoch: 4 Step: 2175 Val Loss: 1.430686116218567\n",
            "Epoch: 4 Step: 2200 Val Loss: 1.4273180961608887\n",
            "Epoch: 4 Step: 2225 Val Loss: 1.4252785444259644\n",
            "Epoch: 4 Step: 2250 Val Loss: 1.4214407205581665\n",
            "Epoch: 4 Step: 2275 Val Loss: 1.4215943813323975\n",
            "Epoch: 4 Step: 2300 Val Loss: 1.4220343828201294\n",
            "Epoch: 4 Step: 2325 Val Loss: 1.426393985748291\n",
            "Epoch: 4 Step: 2350 Val Loss: 1.4248520135879517\n",
            "Epoch: 4 Step: 2375 Val Loss: 1.4232271909713745\n",
            "Epoch: 4 Step: 2400 Val Loss: 1.4176034927368164\n",
            "Epoch: 4 Step: 2425 Val Loss: 1.4180549383163452\n",
            "Epoch: 4 Step: 2450 Val Loss: 1.41218900680542\n",
            "Epoch: 5 Step: 2475 Val Loss: 1.4118642807006836\n",
            "Epoch: 5 Step: 2500 Val Loss: 1.4109165668487549\n",
            "Epoch: 5 Step: 2525 Val Loss: 1.4085361957550049\n",
            "Epoch: 5 Step: 2550 Val Loss: 1.4071203470230103\n",
            "Epoch: 5 Step: 2575 Val Loss: 1.400746464729309\n",
            "Epoch: 5 Step: 2600 Val Loss: 1.4020230770111084\n",
            "Epoch: 5 Step: 2625 Val Loss: 1.3988696336746216\n",
            "Epoch: 5 Step: 2650 Val Loss: 1.3907856941223145\n",
            "Epoch: 5 Step: 2675 Val Loss: 1.3877168893814087\n",
            "Epoch: 5 Step: 2700 Val Loss: 1.3891681432724\n",
            "Epoch: 5 Step: 2725 Val Loss: 1.3915832042694092\n",
            "Epoch: 5 Step: 2750 Val Loss: 1.3910548686981201\n",
            "Epoch: 5 Step: 2775 Val Loss: 1.391298770904541\n",
            "Epoch: 5 Step: 2800 Val Loss: 1.392741084098816\n",
            "Epoch: 5 Step: 2825 Val Loss: 1.3920756578445435\n",
            "Epoch: 5 Step: 2850 Val Loss: 1.3892617225646973\n",
            "Epoch: 5 Step: 2875 Val Loss: 1.3910037279129028\n",
            "Epoch: 5 Step: 2900 Val Loss: 1.3867619037628174\n",
            "Epoch: 5 Step: 2925 Val Loss: 1.3870339393615723\n",
            "Epoch: 6 Step: 2950 Val Loss: 1.3832930326461792\n",
            "Epoch: 6 Step: 2975 Val Loss: 1.3844473361968994\n",
            "Epoch: 6 Step: 3000 Val Loss: 1.384371042251587\n",
            "Epoch: 6 Step: 3025 Val Loss: 1.381149411201477\n",
            "Epoch: 6 Step: 3050 Val Loss: 1.3803627490997314\n",
            "Epoch: 6 Step: 3075 Val Loss: 1.3748958110809326\n",
            "Epoch: 6 Step: 3100 Val Loss: 1.3744621276855469\n",
            "Epoch: 6 Step: 3125 Val Loss: 1.3678178787231445\n",
            "Epoch: 6 Step: 3150 Val Loss: 1.3665720224380493\n",
            "Epoch: 6 Step: 3175 Val Loss: 1.3629788160324097\n",
            "Epoch: 6 Step: 3200 Val Loss: 1.3623119592666626\n",
            "Epoch: 6 Step: 3225 Val Loss: 1.365766167640686\n",
            "Epoch: 6 Step: 3250 Val Loss: 1.368215560913086\n",
            "Epoch: 6 Step: 3275 Val Loss: 1.3635715246200562\n",
            "Epoch: 6 Step: 3300 Val Loss: 1.3722339868545532\n",
            "Epoch: 6 Step: 3325 Val Loss: 1.3726274967193604\n",
            "Epoch: 6 Step: 3350 Val Loss: 1.3677467107772827\n",
            "Epoch: 6 Step: 3375 Val Loss: 1.3657646179199219\n",
            "Epoch: 6 Step: 3400 Val Loss: 1.365123987197876\n",
            "Epoch: 6 Step: 3425 Val Loss: 1.3630269765853882\n",
            "Epoch: 7 Step: 3450 Val Loss: 1.3628170490264893\n",
            "Epoch: 7 Step: 3475 Val Loss: 1.3645938634872437\n",
            "Epoch: 7 Step: 3500 Val Loss: 1.3653416633605957\n",
            "Epoch: 7 Step: 3525 Val Loss: 1.3570938110351562\n",
            "Epoch: 7 Step: 3550 Val Loss: 1.359247088432312\n",
            "Epoch: 7 Step: 3575 Val Loss: 1.3594342470169067\n",
            "Epoch: 7 Step: 3600 Val Loss: 1.3556631803512573\n",
            "Epoch: 7 Step: 3625 Val Loss: 1.3528287410736084\n",
            "Epoch: 7 Step: 3650 Val Loss: 1.3516440391540527\n",
            "Epoch: 7 Step: 3675 Val Loss: 1.3524137735366821\n",
            "Epoch: 7 Step: 3700 Val Loss: 1.3497637510299683\n",
            "Epoch: 7 Step: 3725 Val Loss: 1.3469035625457764\n",
            "Epoch: 7 Step: 3750 Val Loss: 1.3461787700653076\n",
            "Epoch: 7 Step: 3775 Val Loss: 1.3463906049728394\n",
            "Epoch: 7 Step: 3800 Val Loss: 1.352155089378357\n",
            "Epoch: 7 Step: 3825 Val Loss: 1.3559564352035522\n",
            "Epoch: 7 Step: 3850 Val Loss: 1.3540273904800415\n",
            "Epoch: 7 Step: 3875 Val Loss: 1.3461941480636597\n",
            "Epoch: 7 Step: 3900 Val Loss: 1.3464995622634888\n",
            "Epoch: 8 Step: 3925 Val Loss: 1.349524736404419\n",
            "Epoch: 8 Step: 3950 Val Loss: 1.3492504358291626\n",
            "Epoch: 8 Step: 3975 Val Loss: 1.3466414213180542\n",
            "Epoch: 8 Step: 4000 Val Loss: 1.3517571687698364\n",
            "Epoch: 8 Step: 4025 Val Loss: 1.3444912433624268\n",
            "Epoch: 8 Step: 4050 Val Loss: 1.3408156633377075\n",
            "Epoch: 8 Step: 4075 Val Loss: 1.342460036277771\n",
            "Epoch: 8 Step: 4100 Val Loss: 1.345641016960144\n",
            "Epoch: 8 Step: 4125 Val Loss: 1.3384597301483154\n",
            "Epoch: 8 Step: 4150 Val Loss: 1.334957480430603\n",
            "Epoch: 8 Step: 4175 Val Loss: 1.3392406702041626\n",
            "Epoch: 8 Step: 4200 Val Loss: 1.3413704633712769\n",
            "Epoch: 8 Step: 4225 Val Loss: 1.3313649892807007\n",
            "Epoch: 8 Step: 4250 Val Loss: 1.3373723030090332\n",
            "Epoch: 8 Step: 4275 Val Loss: 1.3432178497314453\n",
            "Epoch: 8 Step: 4300 Val Loss: 1.342764139175415\n",
            "Epoch: 8 Step: 4325 Val Loss: 1.3467166423797607\n",
            "Epoch: 8 Step: 4350 Val Loss: 1.3481701612472534\n",
            "Epoch: 8 Step: 4375 Val Loss: 1.344177007675171\n",
            "Epoch: 8 Step: 4400 Val Loss: 1.3439693450927734\n",
            "Epoch: 9 Step: 4425 Val Loss: 1.3387023210525513\n",
            "Epoch: 9 Step: 4450 Val Loss: 1.336743950843811\n",
            "Epoch: 9 Step: 4475 Val Loss: 1.3377439975738525\n",
            "Epoch: 9 Step: 4500 Val Loss: 1.334383249282837\n",
            "Epoch: 9 Step: 4525 Val Loss: 1.3362783193588257\n",
            "Epoch: 9 Step: 4550 Val Loss: 1.3344343900680542\n",
            "Epoch: 9 Step: 4575 Val Loss: 1.3324857950210571\n",
            "Epoch: 9 Step: 4600 Val Loss: 1.331276297569275\n",
            "Epoch: 9 Step: 4625 Val Loss: 1.3313705921173096\n",
            "Epoch: 9 Step: 4650 Val Loss: 1.3331230878829956\n",
            "Epoch: 9 Step: 4675 Val Loss: 1.3262220621109009\n",
            "Epoch: 9 Step: 4700 Val Loss: 1.3350902795791626\n",
            "Epoch: 9 Step: 4725 Val Loss: 1.3283698558807373\n",
            "Epoch: 9 Step: 4750 Val Loss: 1.3265436887741089\n",
            "Epoch: 9 Step: 4775 Val Loss: 1.336440086364746\n",
            "Epoch: 9 Step: 4800 Val Loss: 1.336519479751587\n",
            "Epoch: 9 Step: 4825 Val Loss: 1.3370975255966187\n",
            "Epoch: 9 Step: 4850 Val Loss: 1.3268067836761475\n",
            "Epoch: 9 Step: 4875 Val Loss: 1.3268605470657349\n",
            "Epoch: 9 Step: 4900 Val Loss: 1.3305106163024902\n",
            "Epoch: 10 Step: 4925 Val Loss: 1.330869197845459\n",
            "Epoch: 10 Step: 4950 Val Loss: 1.3345764875411987\n",
            "Epoch: 10 Step: 4975 Val Loss: 1.3330892324447632\n",
            "Epoch: 10 Step: 5000 Val Loss: 1.3313536643981934\n",
            "Epoch: 10 Step: 5025 Val Loss: 1.3253475427627563\n",
            "Epoch: 10 Step: 5050 Val Loss: 1.3269850015640259\n",
            "Epoch: 10 Step: 5075 Val Loss: 1.3251315355300903\n",
            "Epoch: 10 Step: 5100 Val Loss: 1.3250724077224731\n",
            "Epoch: 10 Step: 5125 Val Loss: 1.3215312957763672\n",
            "Epoch: 10 Step: 5150 Val Loss: 1.3220304250717163\n",
            "Epoch: 10 Step: 5175 Val Loss: 1.3260538578033447\n",
            "Epoch: 10 Step: 5200 Val Loss: 1.3268400430679321\n",
            "Epoch: 10 Step: 5225 Val Loss: 1.323641061782837\n",
            "Epoch: 10 Step: 5250 Val Loss: 1.3285595178604126\n",
            "Epoch: 10 Step: 5275 Val Loss: 1.331653118133545\n",
            "Epoch: 10 Step: 5300 Val Loss: 1.3349746465682983\n",
            "Epoch: 10 Step: 5325 Val Loss: 1.3305296897888184\n",
            "Epoch: 10 Step: 5350 Val Loss: 1.3262567520141602\n",
            "Epoch: 10 Step: 5375 Val Loss: 1.3290921449661255\n",
            "Epoch: 11 Step: 5400 Val Loss: 1.3331938982009888\n",
            "Epoch: 11 Step: 5425 Val Loss: 1.3247241973876953\n",
            "Epoch: 11 Step: 5450 Val Loss: 1.324659824371338\n",
            "Epoch: 11 Step: 5475 Val Loss: 1.322105884552002\n",
            "Epoch: 11 Step: 5500 Val Loss: 1.3237444162368774\n",
            "Epoch: 11 Step: 5525 Val Loss: 1.3244869709014893\n",
            "Epoch: 11 Step: 5550 Val Loss: 1.322797417640686\n",
            "Epoch: 11 Step: 5575 Val Loss: 1.320452094078064\n",
            "Epoch: 11 Step: 5600 Val Loss: 1.3141326904296875\n",
            "Epoch: 11 Step: 5625 Val Loss: 1.3171284198760986\n",
            "Epoch: 11 Step: 5650 Val Loss: 1.316116452217102\n",
            "Epoch: 11 Step: 5675 Val Loss: 1.315125823020935\n",
            "Epoch: 11 Step: 5700 Val Loss: 1.3126579523086548\n",
            "Epoch: 11 Step: 5725 Val Loss: 1.315327525138855\n",
            "Epoch: 11 Step: 5750 Val Loss: 1.324768304824829\n",
            "Epoch: 11 Step: 5775 Val Loss: 1.3247926235198975\n",
            "Epoch: 11 Step: 5800 Val Loss: 1.3248002529144287\n",
            "Epoch: 11 Step: 5825 Val Loss: 1.3227189779281616\n",
            "Epoch: 11 Step: 5850 Val Loss: 1.3231348991394043\n",
            "Epoch: 11 Step: 5875 Val Loss: 1.3149136304855347\n",
            "Epoch: 12 Step: 5900 Val Loss: 1.3210854530334473\n",
            "Epoch: 12 Step: 5925 Val Loss: 1.3207188844680786\n",
            "Epoch: 12 Step: 5950 Val Loss: 1.3209408521652222\n",
            "Epoch: 12 Step: 5975 Val Loss: 1.3175694942474365\n",
            "Epoch: 12 Step: 6000 Val Loss: 1.3165398836135864\n",
            "Epoch: 12 Step: 6025 Val Loss: 1.3203861713409424\n",
            "Epoch: 12 Step: 6050 Val Loss: 1.3171634674072266\n",
            "Epoch: 12 Step: 6075 Val Loss: 1.3124443292617798\n",
            "Epoch: 12 Step: 6100 Val Loss: 1.31554114818573\n",
            "Epoch: 12 Step: 6125 Val Loss: 1.3132721185684204\n",
            "Epoch: 12 Step: 6150 Val Loss: 1.3117072582244873\n",
            "Epoch: 12 Step: 6175 Val Loss: 1.3139116764068604\n",
            "Epoch: 12 Step: 6200 Val Loss: 1.308200716972351\n",
            "Epoch: 12 Step: 6225 Val Loss: 1.3185619115829468\n",
            "Epoch: 12 Step: 6250 Val Loss: 1.3176876306533813\n",
            "Epoch: 12 Step: 6275 Val Loss: 1.3172340393066406\n",
            "Epoch: 12 Step: 6300 Val Loss: 1.3205373287200928\n",
            "Epoch: 12 Step: 6325 Val Loss: 1.3150534629821777\n",
            "Epoch: 12 Step: 6350 Val Loss: 1.3193680047988892\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model_name = 'hidden512_layers3_sha.net'\n",
        "# torch.save(model.state_dict(),'/content/drive/MyDrive/Data_sets/Models/'+model_name)"
      ],
      "metadata": {
        "id": "Qm4tM7QDZ40p"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.load_state_dict(torch.load('/content/drive/MyDrive/Data_sets/Models/hidden512_layers3_sha.net'))"
      ],
      "metadata": {
        "id": "Dfhp5Gzfue4d"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Now we will predict new characters"
      ],
      "metadata": {
        "id": "wx0rXTLhsy4f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_next_char(model, char, hidden = None, k = 1):\n",
        "  encoded_text = model.encoder[char]\n",
        "  encoded_text = np.array([[encoded_text]])\n",
        "  encoded_text = one_hot_encoder(encoded_text, len(model.all_chars))\n",
        "\n",
        "  inputs = torch.from_numpy(encoded_text)\n",
        "\n",
        "  if model.use_gpu:\n",
        "    inputs = inputs.cuda()\n",
        "  \n",
        "  hidden = tuple([state.data for state in hidden])\n",
        "\n",
        "  lstm_out, hidden = model(inputs,hidden)\n",
        "\n",
        "  probs = F.softmax(lstm_out, dim = 1).data\n",
        "\n",
        "  if model.use_gpu:\n",
        "\n",
        "    probs = probs.cpu()\n",
        "\n",
        "  probs, index_positions = probs.topk(k)\n",
        "\n",
        "  index_positions = index_positions.numpy().squeeze()\n",
        "\n",
        "  probs = probs.numpy().flatten()\n",
        "\n",
        "  probs = probs/probs.sum()\n",
        "\n",
        "  char = np.random.choice(index_positions, p = probs)\n",
        "\n",
        "  return model.decoder[char], hidden"
      ],
      "metadata": {
        "id": "9Q9LikdxswHI"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(model,size, seed = 'The', k=1):\n",
        "\n",
        "  if model.use_gpu:\n",
        "    model.cuda()\n",
        "  else:\n",
        "    model.cpu()\n",
        "\n",
        "  output_chars = [c for c in seed]\n",
        "\n",
        "  hidden = model.hidden_state(1)\n",
        "\n",
        "  for char in seed:\n",
        "\n",
        "    char,hidden = predict_next_char(model, char, hidden, k = k)\n",
        "  output_chars.append(char)\n",
        "\n",
        "  for i in range(size):\n",
        "\n",
        "    char,hidden = predict_next_char(model, output_chars[-1], hidden, k=k)\n",
        "\n",
        "    output_chars.append(char)\n",
        "\n",
        "  return ''.join(output_chars)"
      ],
      "metadata": {
        "id": "BlruMsvVsacR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model, 1000, seed = 'The', k = 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zutoTypQt305",
        "outputId": "9268b337-fde6-4e10-b3be-651485713b19"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thething offended\n",
            "                               and a stople of the consent.\n",
            "                                           Exeint all but AUFIDIUS  \n",
            "    What a good fool will be too think is the trimms that that this\n",
            "    hatch in him to tell me. If they see, that there is thieves and\n",
            "    thruch here. What may I see the sea of his stock and this thought\n",
            "    here of that time? We see all my heart and string off them to the\n",
            "    thing.\n",
            "  PRONPERO. Why, I do not the word to hard; that in his soul,\n",
            "    And we should be to make me with him. If there be\n",
            "    this that I have saint me. Is he? And I warrant you to the\n",
            "    sons?\n",
            "  CLOWN. I will not see her with me.\n",
            "  SEBISTON. I am not with him with the words of your astainted, and,  \n",
            "    that thou wild sport another still to thy first words, whereof\n",
            "    he is this strange and shrink or as a man that hath she stoods.\n",
            "  POINS. It would not be to be the mask that I shall show thy\n",
            "    tongues.\n",
            "  PROSPERO. What's the woman?\n",
            "  SHELECK. I am no more thou will\n"
          ]
        }
      ]
    }
  ]
}